{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPAF1murQq+Ir/1DaUzQylU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o0jT4Nb_YeYd","executionInfo":{"status":"ok","timestamp":1740827359319,"user_tz":-345,"elapsed":47836,"user":{"displayName":"kaushik","userId":"04503306212181183843"}},"outputId":"37ac55e7-415e-4575-bcb8-0949edaacf6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import os\n","import cv2\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, roc_curve, auc\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","import seaborn as sns\n","import torch.nn.functional as F\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# for lstm+resnet\n","\n","# Paths to real and fake video folders\n","DATASET_PATH = \"/content/drive/MyDrive/celeb\"\n","REAL_FOLDER = \"Celeb-real\"\n","FAKE_FOLDER = \"Celeb-synthesis\"\n","\n","# Define data augmentation and preprocessing\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize((128, 128)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomRotation(degrees=10),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3)], p=0.15),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.5], std=[0.5])  # ResNet expects 3-channel normalization\n","])\n","\n","# Function to extract frames from a video\n","def extract_frames(video_path, frame_count=10):\n","    cap = cv2.VideoCapture(video_path)\n","    frames = []\n","    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","    if total_frames < frame_count:\n","        cap.release()\n","        return None  # Skip videos with too few frames\n","\n","    # Compute evenly spaced frame indices but in sequential order\n","    step = max(total_frames // frame_count, 1)\n","    frame_indices = [i * step for i in range(frame_count)]  # Sequential selection\n","\n","    for idx in frame_indices:\n","        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        frame = transform(frame)  # Shape: [C, H, W]\n","        frames.append(frame)\n","\n","    cap.release()\n","\n","    if len(frames) == frame_count:\n","        return torch.stack(frames)  # Shape: [T, C, H, W]\n","    return None\n","\n","\n","# Custom Dataset Class\n","class DeepfakeDataset(Dataset):\n","    def __init__(self, dataset_path, real_folders, fake_folder, frame_count=10, max_fake_videos=600):\n","        self.videos = []\n","        self.labels = []\n","\n","        # Load real videos\n","        for folder in real_folders:\n","            folder_path = os.path.join(dataset_path, folder)\n","            self.videos.extend([(os.path.join(folder_path, f), 0) for f in os.listdir(folder_path)])\n","\n","        # Load fake videos\n","        fake_folder_path = os.path.join(dataset_path, fake_folder)\n","        fake_videos = [(os.path.join(fake_folder_path, f), 1) for f in os.listdir(fake_folder_path)][:max_fake_videos]\n","        self.videos.extend(fake_videos)\n","\n","        self.frame_count = frame_count\n","        self.data = []\n","        self.labels = []\n","\n","        print(\"Processing videos...\")\n","        for video_path, label in tqdm(self.videos):\n","            frames = extract_frames(video_path, self.frame_count)\n","            if frames is not None:\n","                self.data.append(frames)  # Shape: [T, C, H, W]\n","                self.labels.append(label)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        frames = self.data[idx]  # Shape: [T, C, H, W]\n","        label = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return frames, label  # Frames will be fed to ResNet, then to LSTM\n","\n","# Initialize dataset\n","dataset = DeepfakeDataset(DATASET_PATH, REAL_FOLDER, FAKE_FOLDER, frame_count=10, max_fake_videos=600)\n","\n","# Save dataset to Google Drive\n","#torch.save(dataset.data, '/content/drive/MyDrive/extracted_frames_celeb_lstm.pt')\n","torch.save(dataset.labels, '/content/drive/MyDrive/extracted_labels_celeb_lstm.pt')\n","\n","print(\" Frame extraction complete! Dataset saved to Google Drive.\")\n","torch.save(dataset.data, '/content/drive/MyDrive/extracted_frames_celeb_lstm.pt', _use_new_zipfile_serialization=True)\n"],"metadata":{"id":"jzgbBtjMYlaQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#resnet dataloader\n","from torch.utils.data import DataLoader, Dataset, random_split\n","\n","# Paths to pre-extracted frame tensors\n","frames_path = \"/content/drive/MyDrive/extracted_frames_celeb_lstm.pt\"\n","labels_path = \"/content/drive/MyDrive/extracted_labels_celeb_lstm.pt\"\n","\n","class DeepfakeDataset(Dataset):\n","    def __init__(self, frames_path, labels_path):\n","        self.data = torch.load(frames_path, weights_only=True)\n","        self.labels = torch.load(labels_path, weights_only=True)\n","\n","        if len(self.data) != len(self.labels):\n","            raise ValueError(\"Mismatch: Frames and labels have different lengths!\")\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        frames = self.data[idx].float()  # Shape: [T, C, H, W]\n","        label = torch.tensor(self.labels[idx], dtype=torch.long)\n","        return frames, label  # Returned format: ([T, C, H, W], label)\n","\n","# Load dataset\n","dataset = DeepfakeDataset(frames_path, labels_path)\n","\n","# Split dataset\n","train_size = int(0.75 * len(dataset))\n","val_size = int(0.15 * len(dataset))\n","test_size = len(dataset) - train_size - val_size\n","\n","train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n","\n","# DataLoader with correct batch format\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","\n","# Checking Real vs Fake Distribution\n","def count_real_fake(dataset, original_labels):\n","    real = sum(1 for i in dataset.indices if original_labels[i] == 0)\n","    fake = sum(1 for i in dataset.indices if original_labels[i] == 1)\n","    return real, fake\n","\n","train_real, train_fake = count_real_fake(train_dataset, dataset.labels)\n","val_real, val_fake = count_real_fake(val_dataset, dataset.labels)\n","test_real, test_fake = count_real_fake(test_dataset, dataset.labels)\n","\n","print(f\"Train Set - Real: {train_real}, Fake: {train_fake}\")\n","print(f\"Validation Set - Real: {val_real}, Fake: {val_fake}\")\n","print(f\"Test Set - Real: {test_real}, Fake: {test_fake}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6be4PPIiYoP0","executionInfo":{"status":"ok","timestamp":1740829027473,"user_tz":-345,"elapsed":12127,"user":{"displayName":"kaushik","userId":"04503306212181183843"}},"outputId":"35fe2a3b-37f4-428b-d25c-227cb2f88a2f"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Set - Real: 439, Fake: 452\n","Validation Set - Real: 94, Fake: 84\n","Test Set - Real: 56, Fake: 64\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.models as models\n","from torchvision.models import ResNet18_Weights\n","\n","class DeepfakeDetector(nn.Module):\n","    def __init__(self, hidden_dim=256, num_layers=2, num_classes=2, dropout=0.5):\n","        super(DeepfakeDetector, self).__init__()\n","\n","        # Load Pretrained ResNet with Correct Weights\n","        resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n","        self.resnet = nn.Sequential(*list(resnet.children())[:-1])  # Remove FC layer\n","\n","        # Unfreeze ResNet for Fine-Tuning\n","        for param in self.resnet.parameters():\n","            param.requires_grad = True  # Enable gradient updates\n","\n","        # LSTM for Temporal Processing\n","        self.lstm = nn.LSTM(input_size=512, hidden_size=hidden_dim, num_layers=num_layers,\n","                            batch_first=True, bidirectional=True, dropout=dropout)\n","\n","        self.final_dropout = nn.Dropout(p=dropout)\n","\n","        # Fully Connected Layer\n","        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n","\n","        # Resize transformation (Applied before entering the model)\n","        self.resize = transforms.Resize((224, 224))\n","\n","    def forward(self, x):\n","        B, T, C, H, W = x.shape  # (Batch, Time, Channels, Height, Width)\n","\n","        # Reshape and Resize Frames Efficiently\n","        x = x.view(B * T, C, H, W)  # Flatten batch & time\n","        x = self.resize(x)  # Resize to (224, 224)\n","\n","        # Feature Extraction via ResNet\n","        x = self.resnet(x)  # Output: (B*T, 512, 1, 1)\n","        x = x.view(B, T, 512)  # Reshape for LSTM\n","\n","        # LSTM Processing\n","        lstm_out, _ = self.lstm(x)\n","\n","        # Apply Dropout Before Extracting Last Frame's Output\n","        lstm_out = self.final_dropout(lstm_out)\n","        last_out = lstm_out[:, -1, :]  # Extract last timestep\n","\n","        # Classification\n","        output = self.fc(last_out)\n","        return output\n"],"metadata":{"id":"-4IYu0T-hjwm","executionInfo":{"status":"ok","timestamp":1740832886864,"user_tz":-345,"elapsed":4,"user":{"displayName":"kaushik","userId":"04503306212181183843"}}},"execution_count":70,"outputs":[]},{"cell_type":"code","source":["#resnet training loop\n","\n","# Define the device (GPU or CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Initialize model, criterion, and optimizer\n","model = DeepfakeDetector().to(device)  # ResNet + LSTM model\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-5)\n","\n","num_epochs = 10\n","\n","# Lists to store loss and accuracy\n","train_losses, train_accuracies = [], []\n","val_losses, val_accuracies = [], []\n","\n","best_val_loss = float('inf')  # Track the lowest validation loss\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    all_train_preds, all_train_labels = [], []\n","\n","    for frames, labels in tqdm(train_loader):\n","        frames, labels = frames.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(frames)  # Forward pass through ResNet + LSTM\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Collect predictions and labels for accuracy calculation\n","        _, predicted = torch.max(outputs, 1)\n","        all_train_preds.extend(predicted.cpu().numpy())\n","        all_train_labels.extend(labels.cpu().numpy())\n","\n","        running_loss += loss.item()\n","\n","    # Compute training loss and accuracy\n","    train_loss = running_loss / len(train_loader)\n","    train_accuracy = accuracy_score(all_train_labels, all_train_preds) * 100\n","    train_losses.append(train_loss)\n","    train_accuracies.append(train_accuracy)\n","\n","    # Validation loop\n","    model.eval()\n","    running_val_loss = 0.0\n","    all_val_preds, all_val_labels = [], []\n","\n","    with torch.no_grad():\n","        for frames, labels in tqdm(val_loader):\n","            frames, labels = frames.to(device), labels.to(device)\n","            outputs = model(frames)\n","            loss = criterion(outputs, labels)\n","\n","            # Collect predictions and labels for accuracy calculation\n","            _, predicted = torch.max(outputs, 1)\n","            all_val_preds.extend(predicted.cpu().numpy())\n","            all_val_labels.extend(labels.cpu().numpy())\n","\n","            running_val_loss += loss.item()\n","\n","    # Compute validation loss and accuracy\n","    val_loss = running_val_loss / len(val_loader)\n","    val_accuracy = accuracy_score(all_val_labels, all_val_preds) * 100\n","    val_losses.append(val_loss)\n","    val_accuracies.append(val_accuracy)\n","\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n","          f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n","\n","    # **Save the model if this epoch has the lowest validation loss**\n","    if val_loss < best_val_loss:\n","        best_val_loss = val_loss\n","        checkpoint = {\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'train_losses': train_losses,\n","            'val_losses': val_losses,\n","            'train_accuracies': train_accuracies,\n","            'val_accuracies': val_accuracies\n","        }\n","        torch.save(checkpoint, \"resnet_lstm_model_version_4.pth\")\n","        saved_epoch = epoch\n","        print(f\"Model saved at Epoch {epoch+1} with Validation Loss: {best_val_loss:.4f}\")\n","\n","# Function to plot learning curves\n","def plot_learning_curves(train_losses, train_accuracies, val_losses, val_accuracies, saved_epoch):\n","    epochs = range(1, len(train_losses) + 1)\n","\n","    plt.figure(figsize=(12, 6))\n","\n","    # Plot the loss curve\n","    plt.subplot(1, 2, 1)\n","    plt.plot(epochs, train_losses, label='Train Loss', color='blue')\n","    plt.plot(epochs, val_losses, label='Validation Loss', color='red')\n","\n","    if saved_epoch != -1:\n","        plt.axvline(x=saved_epoch+1, color='green', linestyle='--', label=f'Saved at Epoch {saved_epoch+1}')\n","\n","    plt.title('Loss Curve')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","\n","    # Plot the accuracy curve\n","    plt.subplot(1, 2, 2)\n","    plt.plot(epochs, train_accuracies, label='Train Accuracy', color='blue')\n","    plt.plot(epochs, val_accuracies, label='Validation Accuracy', color='red')\n","\n","    plt.title('Accuracy Curve')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy (%)')\n","    plt.legend()\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Plot the learning curves\n","plot_learning_curves(train_losses, train_accuracies, val_losses, val_accuracies, saved_epoch)\n"],"metadata":{"id":"LED70brocZJS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# resnet\n","\n","# Initialize the model (Must match saved architecture)\n","model = DeepfakeDetector().to(device)  # ResNet + LSTM model\n","\n","# Load the best model checkpoint\n","checkpoint = torch.load(\"resnet_lstm_model_version_4.pth\", map_location=device)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","print(f\"ResNet-LSTM Model loaded from checkpoint at Epoch {checkpoint['epoch'] + 1}\")\n"],"metadata":{"id":"Zv2bQarqdF-m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# resnet evaluation\n","\n","# Function to evaluate model performance\n","def evaluate(model, dataloader, device):\n","    model.eval()  # Set model to evaluation mode\n","    all_labels = []\n","    all_probs = []\n","    all_preds = []\n","\n","    with torch.no_grad():\n","        for frames, labels in dataloader:\n","            frames, labels = frames.to(device), labels.to(device)\n","\n","            # Forward pass through ResNet + LSTM model\n","            outputs = model(frames)\n","\n","            # Apply softmax to get class probabilities\n","            probs = torch.nn.functional.softmax(outputs, dim=1)\n","            _, predicted = torch.max(probs, 1)  # Get predicted labels\n","\n","            # Store results for evaluation\n","            all_labels.extend(labels.cpu().numpy())\n","            all_preds.extend(predicted.cpu().numpy())\n","            all_probs.extend(probs[:, 1].cpu().numpy())  # Probability of class '1' (Fake)\n","\n","    # Convert lists to NumPy arrays\n","    all_labels = np.array(all_labels)\n","    all_preds = np.array(all_preds)\n","    all_probs = np.array(all_probs)\n","\n","    # Compute evaluation metrics\n","    accuracy = accuracy_score(all_labels, all_preds) * 100\n","    f1 = f1_score(all_labels, all_preds, average='binary')\n","    conf_matrix = confusion_matrix(all_labels, all_preds)\n","\n","    # Compute ROC curve and AUC\n","    fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n","    roc_auc = auc(fpr, tpr)\n","\n","    return accuracy, f1, conf_matrix, fpr, tpr, thresholds, roc_auc, all_labels, all_probs\n","\n","# Load best model checkpoint\n","checkpoint = torch.load(\"resnet_lstm_model_version_4.pth\", map_location=device)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","print(f\" ResNet-LSTM Model loaded from Epoch {checkpoint['epoch'] + 1}\")\n","\n","# Run evaluation on test dataset\n","test_acc, test_f1, test_conf_matrix, fpr, tpr, thresholds, roc_auc, all_labels, all_probs = evaluate(model, test_loader, device)\n","\n","# Print results\n","print(f\" Test Accuracy: {test_acc:.2f}%\")\n","print(f\" Test F1 Score: {test_f1:.2f}\")\n","print(f\" Confusion Matrix:\\n{test_conf_matrix}\")\n","print(f\" ROC AUC: {roc_auc:.2f}\")\n"],"metadata":{"id":"VV3xHIA6dKzo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# resnet visualization\n","from sklearn.metrics import f1_score as sklearn_f1_score  # Avoiding name conflicts\n","\n","# Function to plot evaluation metrics for ResNet + LSTM\n","def plot_evaluation_metrics(accuracy, f1_score, conf_matrix, fpr, tpr, thresholds, roc_auc, all_labels, all_probs):\n","    plt.figure(figsize=(12, 6))\n","\n","    # Bar Plot for Accuracy and F1 Score\n","    plt.subplot(1, 2, 1)\n","    plt.bar([\"Accuracy\", \"F1 Score\"], [accuracy, f1_score], color=[\"green\", \"blue\"])\n","    plt.ylim(0, 100)\n","    plt.xlabel(\"Metrics\")\n","    plt.ylabel(\"Score (%)\")\n","    plt.title(\"Model Performance Metrics\")\n","\n","    # Confusion Matrix Plot\n","    plt.subplot(1, 2, 2)\n","    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Real\", \"Fake\"], yticklabels=[\"Real\", \"Fake\"])\n","    plt.xlabel(\"Predicted\")\n","    plt.ylabel(\"Actual\")\n","    plt.title(\"Confusion Matrix\")\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # Plot ROC Curve\n","    plt.figure(figsize=(6, 6))\n","    plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n","    plt.plot([0, 1], [0, 1], color='gray', linestyle=\"--\")  # Diagonal line for reference\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel(\"False Positive Rate\")\n","    plt.ylabel(\"True Positive Rate\")\n","    plt.title(\"ROC Curve\")\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","    # Compute F1 Score for different thresholds\n","    threshold_range = np.arange(0.0, 1.05, 0.05)\n","    f1_scores = [sklearn_f1_score(all_labels, (all_probs >= threshold).astype(int), average=\"binary\") for threshold in threshold_range]\n","\n","    # Plot F1 Score vs Threshold\n","    plt.figure(figsize=(6, 6))\n","    plt.plot(threshold_range, f1_scores, color=\"red\", lw=2)\n","    plt.xlabel(\"Threshold\")\n","    plt.ylabel(\"F1 Score\")\n","    plt.title(\"F1 Score at Different Thresholds\")\n","    plt.grid(True)\n","    plt.show()\n","\n","plt.tight_layout()\n","plt.show()\n","\n","\n","# Load best ResNet + LSTM checkpoint\n","checkpoint = torch.load(\"resnet_lstm_model_version_4.pth\", map_location=device)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","print(f\" ResNet-LSTM Model loaded from Epoch {checkpoint['epoch'] + 1}\")\n","\n","# Run evaluation on test dataset\n","test_acc, test_f1, test_conf_matrix, fpr, tpr, thresholds, roc_auc, all_labels, all_probs = evaluate(model, test_loader, device)\n","\n","# Plot the evaluation results\n","plot_evaluation_metrics(test_acc, test_f1, test_conf_matrix, fpr, tpr, thresholds, roc_auc, all_labels, all_probs)\n"],"metadata":{"id":"YKFB2L1xdMyx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import shutil\n","\n","# Define the path to save the model in Google Drive\n","model_path = \"/content/drive/My Drive/resnet_lstm_model_version_4.pth\"\n","\n","# Copy model from Colab storage to Google Drive\n","shutil.copy(\"resnet_lstm_model_version_4.pth\", model_path)\n","\n","print(f\" Model saved in Google Drive: {model_path}\")\n"],"metadata":{"id":"R0xDmBQ4dO5Q"},"execution_count":null,"outputs":[]}]}